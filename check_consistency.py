import os
import pandas as pd
import pickle
import torch
import numpy as np
from config import Config
from processed_dataset import ProcessedQRTDataset, load_target_mapping

def check_file_existence():
    """æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    print("=== æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥ ===")
    
    required_files = [
        'train_processed.csv',
        'test_processed.csv', 
        'target_mapping.pkl',
        'imputer.pkl',
        'scaler.pkl'
    ]
    
    missing_files = []
    for file in required_files:
        if os.path.exists(file):
            size = os.path.getsize(file) / (1024 * 1024)  # MB
            print(f"âœ“ {file} ({size:.1f} MB)")
        else:
            print(f"âœ— {file} - ç¼ºå¤±")
            missing_files.append(file)
    
    if missing_files:
        print(f"\nè­¦å‘Š: ç¼ºå°‘ {len(missing_files)} ä¸ªæ–‡ä»¶")
        return False
    else:
        print("\nâœ“ æ‰€æœ‰å¿…è¦æ–‡ä»¶éƒ½å­˜åœ¨")
        return True

def check_data_consistency():
    """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
    print("\n=== æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥ ===")
    
    # åŠ è½½å¤„ç†åçš„æ•°æ®
    try:
        train_data = pd.read_csv('train_processed.csv')
        test_data = pd.read_csv('test_processed.csv')
        
        print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_data.shape}")
        print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_data.shape}")
        
        # æ£€æŸ¥ç‰¹å¾åˆ—ï¼ˆæ’é™¤RET_TARGETï¼‰
        train_features = [col for col in train_data.columns if col.startswith('RET_') and col != 'RET_TARGET']
        test_features = [col for col in test_data.columns if col.startswith('RET_')]
        
        print(f"è®­ç»ƒæ•°æ®ç‰¹å¾æ•°: {len(train_features)}")
        print(f"æµ‹è¯•æ•°æ®ç‰¹å¾æ•°: {len(test_features)}")
        
        # æ£€æŸ¥ç‰¹å¾åˆ—æ˜¯å¦ä¸€è‡´
        if train_features == test_features:
            print("âœ“ è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ç‰¹å¾åˆ—ä¸€è‡´")
        else:
            print("âœ— è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ç‰¹å¾åˆ—ä¸ä¸€è‡´")
            return False
        
        # æ£€æŸ¥è®­ç»ƒæ•°æ®çš„ç›®æ ‡å˜é‡
        if 'target_idx' in train_data.columns:
            target_range = train_data['target_idx'].min(), train_data['target_idx'].max()
            print(f"ç›®æ ‡ç´¢å¼•èŒƒå›´: {target_range}")
            
            if target_range == (0, 99):
                print("âœ“ ç›®æ ‡ç´¢å¼•èŒƒå›´æ­£ç¡® (0-99)")
            else:
                print(f"âœ— ç›®æ ‡ç´¢å¼•èŒƒå›´å¼‚å¸¸: {target_range}")
                return False
        
        return True
        
    except Exception as e:
        print(f"âœ— æ•°æ®åŠ è½½é”™è¯¯: {e}")
        return False

def check_target_mapping():
    """æ£€æŸ¥ç›®æ ‡æ˜ å°„"""
    print("\n=== ç›®æ ‡æ˜ å°„æ£€æŸ¥ ===")
    
    try:
        target_to_idx, idx_to_target = load_target_mapping()
        
        print(f"æ˜ å°„å­—å…¸å¤§å°: {len(target_to_idx)}")
        print(f"æ˜ å°„èŒƒå›´: {min(target_to_idx.values())} - {max(target_to_idx.values())}")
        
        # æ£€æŸ¥æ˜ å°„çš„ä¸€è‡´æ€§
        for target, idx in target_to_idx.items():
            if idx_to_target[idx] != target:
                print(f"âœ— æ˜ å°„ä¸ä¸€è‡´: {target} -> {idx} -> {idx_to_target[idx]}")
                return False
        
        print("âœ“ ç›®æ ‡æ˜ å°„ä¸€è‡´")
        return True
        
    except Exception as e:
        print(f"âœ— ç›®æ ‡æ˜ å°„åŠ è½½é”™è¯¯: {e}")
        return False

def check_preprocessors():
    """æ£€æŸ¥é¢„å¤„ç†å™¨"""
    print("\n=== é¢„å¤„ç†å™¨æ£€æŸ¥ ===")
    
    try:
        # åŠ è½½é¢„å¤„ç†å™¨
        with open('imputer.pkl', 'rb') as f:
            imputer = pickle.load(f)
        
        with open('scaler.pkl', 'rb') as f:
            scaler = pickle.load(f)
        
        print(f"ç¼ºå¤±å€¼å¡«å……å™¨: {type(imputer).__name__}")
        print(f"ç‰¹å¾æ ‡å‡†åŒ–å™¨: {type(scaler).__name__}")
        
        # æ£€æŸ¥é¢„å¤„ç†å™¨å‚æ•°
        if hasattr(imputer, 'statistics_'):
            print(f"å¡«å……å™¨ç»Ÿè®¡ä¿¡æ¯å½¢çŠ¶: {imputer.statistics_.shape}")
        
        if hasattr(scaler, 'mean_'):
            print(f"æ ‡å‡†åŒ–å™¨å‡å€¼å½¢çŠ¶: {scaler.mean_.shape}")
            print(f"æ ‡å‡†åŒ–å™¨æ ‡å‡†å·®å½¢çŠ¶: {scaler.scale_.shape}")
        
        print("âœ“ é¢„å¤„ç†å™¨åŠ è½½æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âœ— é¢„å¤„ç†å™¨åŠ è½½é”™è¯¯: {e}")
        return False

def check_dataset_compatibility():
    """æ£€æŸ¥æ•°æ®é›†å…¼å®¹æ€§"""
    print("\n=== æ•°æ®é›†å…¼å®¹æ€§æ£€æŸ¥ ===")
    
    try:
        # æµ‹è¯•è®­ç»ƒæ•°æ®é›†
        train_dataset = ProcessedQRTDataset('train_processed.csv', is_training=True)
        print(f"è®­ç»ƒæ•°æ®é›†æ ·æœ¬æ•°: {len(train_dataset)}")
        print(f"è¾“å…¥ç»´åº¦: {train_dataset.input_dim}")
        
        # æµ‹è¯•ä¸€ä¸ªæ ·æœ¬
        sample = train_dataset[0]
        if len(sample) == 4:
            X, target_idx, target_sign, weight = sample
            print(f"æ ·æœ¬æ ¼å¼æ­£ç¡®: X({X.shape}), target_idx({target_idx}), target_sign({target_sign}), weight({weight})")
        else:
            print(f"âœ— æ ·æœ¬æ ¼å¼é”™è¯¯: {len(sample)} ä¸ªå…ƒç´ ")
            return False
        
        # æ£€æŸ¥ç›®æ ‡ç¬¦å·åˆ†å¸ƒï¼ˆä¿®å¤è´Ÿå€¼é—®é¢˜ï¼‰
        try:
            unique_signs, counts = np.unique(train_dataset.target_sign, return_counts=True)
            sign_dist = dict(zip(unique_signs, counts))
            print(f"ç›®æ ‡ç¬¦å·åˆ†å¸ƒ: {sign_dist}")
        except Exception as e:
            print(f"ç›®æ ‡ç¬¦å·åˆ†å¸ƒæ£€æŸ¥å¤±è´¥: {e}")
            return False
        
        # æµ‹è¯•æµ‹è¯•æ•°æ®é›†
        test_dataset = ProcessedQRTDataset('test_processed.csv', is_training=False)
        print(f"æµ‹è¯•æ•°æ®é›†æ ·æœ¬æ•°: {len(test_dataset)}")
        print(f"è¾“å…¥ç»´åº¦: {test_dataset.input_dim}")
        
        # æµ‹è¯•ä¸€ä¸ªæ ·æœ¬
        sample = test_dataset[0]
        if isinstance(sample, np.ndarray):
            print(f"æµ‹è¯•æ ·æœ¬æ ¼å¼æ­£ç¡®: X({sample.shape})")
        else:
            print(f"âœ— æµ‹è¯•æ ·æœ¬æ ¼å¼é”™è¯¯: {type(sample)}")
            return False
        
        print("âœ“ æ•°æ®é›†å…¼å®¹æ€§æ£€æŸ¥é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— æ•°æ®é›†å…¼å®¹æ€§æ£€æŸ¥å¤±è´¥: {e}")
        return False

def check_config_consistency():
    """æ£€æŸ¥é…ç½®ä¸€è‡´æ€§"""
    print("\n=== é…ç½®ä¸€è‡´æ€§æ£€æŸ¥ ===")
    
    try:
        config = Config()
        
        # æ£€æŸ¥é…ç½®ä¸­çš„è·¯å¾„
        if os.path.exists(config.train_processed_file):
            print(f"âœ“ è®­ç»ƒæ•°æ®è·¯å¾„æ­£ç¡®: {config.train_processed_file}")
        else:
            print(f"âœ— è®­ç»ƒæ•°æ®è·¯å¾„é”™è¯¯: {config.train_processed_file}")
            return False
        
        if os.path.exists(config.test_processed_file):
            print(f"âœ“ æµ‹è¯•æ•°æ®è·¯å¾„æ­£ç¡®: {config.test_processed_file}")
        else:
            print(f"âœ— æµ‹è¯•æ•°æ®è·¯å¾„é”™è¯¯: {config.test_processed_file}")
            return False
        
        # æ£€æŸ¥æ¨¡å‹é…ç½®
        print(f"æ¨¡å‹ç±»å‹: {config.model_type}")
        print(f"éšè—å±‚ç»´åº¦: {config.hidden_dims}")
        print(f"ç›®æ ‡å¤´æ•°: {config.num_heads}")
        print(f"æ‰¹æ¬¡å¤§å°: {config.batch_size}")
        print(f"å­¦ä¹ ç‡: {config.learning_rate}")
        
        print("âœ“ é…ç½®ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— é…ç½®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æ£€æŸ¥å‡½æ•°"""
    print("å¼€å§‹é¡¹ç›®ä¸€è‡´æ€§æ£€æŸ¥...\n")
    
    checks = [
        check_file_existence,
        check_data_consistency,
        check_target_mapping,
        check_preprocessors,
        check_dataset_compatibility,
        check_config_consistency
    ]
    
    results = []
    for check in checks:
        try:
            result = check()
            results.append(result)
        except Exception as e:
            print(f"æ£€æŸ¥å¤±è´¥: {e}")
            results.append(False)
    
    # æ€»ç»“
    print("\n=== æ£€æŸ¥æ€»ç»“ ===")
    passed = sum(results)
    total = len(results)
    
    print(f"é€šè¿‡æ£€æŸ¥: {passed}/{total}")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ£€æŸ¥éƒ½é€šè¿‡äº†ï¼é¡¹ç›®æ–‡ä»¶ä¿æŒä¸€è‡´ã€‚")
        print("\nä¸‹ä¸€æ­¥å¯ä»¥:")
        print("1. è®­ç»ƒæ¨¡å‹: python main.py train")
        print("2. ç”Ÿæˆé¢„æµ‹: python main.py predict")
        print("3. è¯„ä¼°æ¨¡å‹: python main.py evaluate")
    else:
        print("âš ï¸  éƒ¨åˆ†æ£€æŸ¥æœªé€šè¿‡ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é—®é¢˜ã€‚")
    
    return passed == total

if __name__ == "__main__":
    main() 