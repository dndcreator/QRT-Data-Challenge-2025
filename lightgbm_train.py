import pandas as pd
from lightgbm import LGBMClassifier
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, roc_auc_score
import lightgbm as lgb
import numpy as np
import optuna

# QRT Score è®¡ç®—å‡½æ•°
def qrt_score(y_true, y_pred, sample_weight):
    return np.sum((y_true == y_pred) * sample_weight) / np.sum(sample_weight)

# 1. è¯»å–æ•°æ®
train = pd.read_csv('train_processed.csv')
test = pd.read_csv('test_processed.csv')

# 2. ç‰¹å¾ä¸æ ‡ç­¾
feature_cols = [col for col in train.columns if col.startswith('RET_') and col != 'RET_TARGET']
industry_cols = [col for col in train.columns if col.startswith('CLASS_LEVEL_')]  # ä¿®æ­£ï¼šCLASS_LEVEL_

# ç¡®ä¿ç‰¹å¾åˆ—åœ¨æµ‹è¯•é›†ä¸­ä¹Ÿå­˜åœ¨
common_cols = [col for col in feature_cols if col in test.columns]
common_industry_cols = [col for col in industry_cols if col in test.columns]

if len(common_cols) != len(feature_cols):
    print(f"âš ï¸ è­¦å‘Š: è®­ç»ƒé›†æœ‰ {len(feature_cols)} ä¸ªæ”¶ç›Šç‡ç‰¹å¾ï¼Œæµ‹è¯•é›†åªæœ‰ {len(common_cols)} ä¸ªå…±åŒç‰¹å¾")
    print(f"ç¼ºå¤±æ”¶ç›Šç‡ç‰¹å¾: {set(feature_cols) - set(test.columns)}")

if len(common_industry_cols) != len(industry_cols):
    print(f"âš ï¸ è­¦å‘Š: è®­ç»ƒé›†æœ‰ {len(industry_cols)} ä¸ªè¡Œä¸šç‰¹å¾ï¼Œæµ‹è¯•é›†åªæœ‰ {len(common_industry_cols)} ä¸ªå…±åŒç‰¹å¾")
    print(f"ç¼ºå¤±è¡Œä¸šç‰¹å¾: {set(industry_cols) - set(test.columns)}")

# ä½¿ç”¨å…±åŒçš„ç‰¹å¾
X = train[common_cols + common_industry_cols]  # åŒ…å«è¡Œä¸šåˆ†ç±»ç‰¹å¾
y = (train['target_sign'] > 0).astype(int)  # 1:æ¶¨, 0:ä¸æ¶¨
sample_weights = train['RET_TARGET'].abs().values  # ä½¿ç”¨RET_TARGETç»å¯¹å€¼ä½œä¸ºæƒé‡

# å¤„ç†åˆ†ç±»ç‰¹å¾ï¼šone-hotç‰¹å¾ä¸éœ€è¦ç‰¹æ®Šå¤„ç†
# for col in common_industry_cols:
#     X[col] = X[col].astype(int)
#     if X[col].min() != 0:
#         print(f"âš ï¸ è°ƒæ•´ {col}: æœ€å°å€¼ä» {X[col].min()} è°ƒæ•´ä¸º 0")
#         X[col] = X[col] - X[col].min()

print(f"ç‰¹å¾ç»Ÿè®¡: æ”¶ç›Šç‡ç‰¹å¾ {len(common_cols)} ä¸ª, è¡Œä¸šç‰¹å¾ {len(common_industry_cols)} ä¸ª")
print(f"è¡Œä¸šç‰¹å¾: {common_industry_cols[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª

# 3. ä½¿ç”¨TimeSeriesSplitè¿›è¡Œæ—¶é—´åºåˆ—äº¤å‰éªŒè¯
tscv = TimeSeriesSplit(n_splits=3)

# åˆ’åˆ†éªŒè¯é›†ï¼ˆä½¿ç”¨æ—¶é—´åºåˆ—åˆ†å‰²ï¼‰
train_idx, val_idx = list(tscv.split(X))[-1]  # ä½¿ç”¨æœ€åä¸€ä¸ªåˆ†å‰²ä½œä¸ºéªŒè¯é›†
X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
train_weights = sample_weights[train_idx]
val_weights = sample_weights[val_idx]

print(f"LightGBMè®­ç»ƒæ•°æ®: {X_train.shape[0]} æ ·æœ¬, {X_train.shape[1]} ç‰¹å¾")
print(f"LightGBMéªŒè¯æ•°æ®: {X_val.shape[0]} æ ·æœ¬")
print(f"æƒé‡ç»Ÿè®¡: è®­ç»ƒé›†å¹³å‡æƒé‡={np.mean(train_weights):.4f}, éªŒè¯é›†å¹³å‡æƒé‡={np.mean(val_weights):.4f}")

# 4. è‡ªåŠ¨è°ƒå‚å‡½æ•°
def tune_lightgbm_params(X_train, X_val, y_train, y_val, train_weights, val_weights, common_industry_cols):
    """ä½¿ç”¨Optunaè‡ªåŠ¨è°ƒå‚LightGBM - ä½¿ç”¨æ ·æœ¬æƒé‡"""
    print("=== å¼€å§‹LightGBMè‡ªåŠ¨è°ƒå‚ (WBCEä¼˜åŒ–) ===")
    
    def objective(trial):
        # å‚è€ƒé‡åŒ–ç«èµ›ä¼˜ç§€åšæ³•ï¼šä¿å®ˆçš„å‚æ•°èŒƒå›´ï¼Œé¿å…è¿‡æ‹Ÿåˆ
        params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'n_estimators': 1000,
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05, log=True),  # æ›´ä¿å®ˆçš„å­¦ä¹ ç‡
            'num_leaves': trial.suggest_int('num_leaves', 31, 127),  # æ›´ä¿å®ˆçš„å¶å­æ•°
            'max_depth': trial.suggest_int('max_depth', 3, 6),  # æ›´ä¿å®ˆçš„æ·±åº¦
            'min_child_samples': trial.suggest_int('min_child_samples', 50, 200),  # æ›´ä¸¥æ ¼çš„è¿‡æ‹Ÿåˆæ§åˆ¶
            'min_child_weight': trial.suggest_float('min_child_weight', 1e-2, 1e-1, log=True),
            'lambda_l1': trial.suggest_float('lambda_l1', 1e-3, 1.0, log=True),  # æ›´ä¿å®ˆçš„æ­£åˆ™åŒ–
            'lambda_l2': trial.suggest_float('lambda_l2', 1e-3, 1.0, log=True),
            'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),  # æ›´ä¿å®ˆçš„ç‰¹å¾é‡‡æ ·
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 5),
            'n_jobs': -1,
            'verbose': -1,
            'random_state': 42
        }
        
        # å‚è€ƒé‡åŒ–ç«èµ›æ ‡å‡†åšæ³•ï¼šä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯é¿å…è¿‡æ‹Ÿåˆ
        tscv = TimeSeriesSplit(n_splits=3)
        
        cv_scores = []
        for train_idx, val_idx in tscv.split(X_train):
            X_cv_train, X_cv_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
            w_cv_train = train_weights[train_idx]  # ä½¿ç”¨å¯¹åº”çš„æƒé‡
            
            clf = LGBMClassifier(**params)
            clf.fit(
                X_cv_train, y_cv_train,
                sample_weight=w_cv_train,  # æ·»åŠ æ ·æœ¬æƒé‡
                eval_set=[(X_cv_val, y_cv_val)],
                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
            )
            
            val_probs = clf.predict_proba(X_cv_val)[:, 1]
            # ä½¿ç”¨QRT Scoreè€Œä¸æ˜¯AUCï¼Œæ›´ç¬¦åˆé‡åŒ–ç«èµ›ç›®æ ‡
            val_pred = (val_probs > 0.5).astype(int) * 2 - 1
            cv_weights = train.loc[X_cv_val.index, 'RET_TARGET'].abs().values
            qrt_score_cv = qrt_score(y_cv_val.values * 2 - 1, val_pred, cv_weights)
            cv_scores.append(qrt_score_cv)
        
        # ä½¿ç”¨äº¤å‰éªŒè¯çš„å¹³å‡QRT Score
        mean_qrt_score = np.mean(cv_scores)
        
        return mean_qrt_score
        

    
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=50, show_progress_bar=True)  # å¢åŠ åˆ°50æ¬¡è¯•éªŒä»¥è·å¾—æœ€ä¼˜ç»“æœ
    
    print(f"LightGBMæœ€ä½³ç»¼åˆè¯„åˆ†: {study.best_value:.4f}")
    print(f"LightGBMæœ€ä½³å‚æ•°: {study.best_params}")
    
    return study.best_params

# 4. ä½¿ç”¨æœ€ä¼˜å‚æ•°ï¼ˆè·³è¿‡è°ƒå‚ï¼‰
print("=== ä½¿ç”¨æœ€ä¼˜å‚æ•°è®­ç»ƒ (WBCEä¼˜åŒ–) ===")
print("ğŸ” ä½¿ç”¨å·²æ‰¾åˆ°çš„æœ€ä¼˜å‚æ•°ç›´æ¥è®­ç»ƒ...")

# ä½¿ç”¨ä¼˜åŒ–åçš„æ¿€è¿›å‚æ•° - æœ€åä¸€æï¼
best_params = {
    'learning_rate': 0.025,  # é™ä½å­¦ä¹ ç‡ï¼Œæ›´ç¨³å®š
    'num_leaves': 127,  # å¢åŠ å¶å­æ•°ï¼Œæ•æ‰æ›´å¤šæ¨¡å¼
    'max_depth': 8,  # å¢åŠ æ·±åº¦
    'min_child_samples': 50,  # é™ä½æœ€å°æ ·æœ¬æ•°
    'min_child_weight': 0.01,  # é™ä½æœ€å°æƒé‡
    'lambda_l1': 0.1,  # å¢åŠ L1æ­£åˆ™åŒ–
    'lambda_l2': 0.01,  # å¢åŠ L2æ­£åˆ™åŒ–
    'feature_fraction': 0.9,  # å¢åŠ ç‰¹å¾é‡‡æ ·
    'bagging_fraction': 0.9,  # å¢åŠ æ ·æœ¬é‡‡æ ·
    'bagging_freq': 3,  # å¢åŠ baggingé¢‘ç‡
    'n_jobs': -1,
    'verbose': -1,
    'random_state': 42,
    'objective': 'binary',
    'metric': 'binary_logloss',
    'boosting_type': 'gbdt'
}

print(f"âœ… ä½¿ç”¨æœ€ä¼˜å‚æ•°: {best_params}")

# 5. è®­ç»ƒæ¨¡å‹ - ä½¿ç”¨æ ·æœ¬æƒé‡
print("å¼€å§‹è®­ç»ƒLightGBMæ¨¡å‹ (ä½¿ç”¨WBCE)...")
clf = LGBMClassifier(**best_params)
clf.fit(
    X_train, y_train,
    sample_weight=train_weights,  # æ·»åŠ æ ·æœ¬æƒé‡
    eval_set=[(X_val, y_val)],
    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
)

# 6. éªŒè¯é›†è¯„ä¼°
val_probs = clf.predict_proba(X_val)[:, 1]
y_val_true = y_val.values

# è°ƒä¼˜é˜ˆå€¼
best_score = -1
best_thresh = 0.5
for thresh in np.arange(0.1, 0.9, 0.01):
    val_pred = (val_probs > thresh).astype(int) * 2 - 1
    score = qrt_score(y_val_true * 2 - 1, val_pred, val_weights)
    if score > best_score:
        best_score = score
        best_thresh = thresh

print(f'æœ€ä¼˜é˜ˆå€¼: {best_thresh:.2f}, éªŒè¯é›†QRT Score: {best_score:.4f}')

# 7. æµ‹è¯•é›†é¢„æµ‹
X_test = test[common_cols + common_industry_cols]
# ç¡®ä¿æµ‹è¯•é›†çš„åˆ†ç±»ç‰¹å¾å€¼ä¸è®­ç»ƒé›†ä¸€è‡´
# for col in common_industry_cols:
#     X_test[col] = X_test[col].astype(int)
#     if X_test[col].min() != 0:
#         X_test[col] = X_test[col] - X_test[col].min()

test_probs = clf.predict_proba(X_test)[:, 1]
test_pred = (test_probs > best_thresh).astype(int) * 2 - 1

# 8. ä¿å­˜ç»“æœ
submit = pd.DataFrame({'ID': test['ID'], 'RET_TARGET': test_pred})
submit.to_csv('submit_lgb_wbce.csv', index=False)

# ä¿å­˜æ¦‚ç‡æ–‡ä»¶
val_prob_df = pd.DataFrame({
    'ID': X_val.index,
    'lgb_wbce_prob': val_probs
})
val_prob_df.to_csv('lightgbm_wbce_val_probs.csv', index=False)

test_prob_df = pd.DataFrame({
    'ID': test['ID'],
    'lgb_wbce_prob': test_probs
})
test_prob_df.to_csv('lightgbm_wbce_test_probs.csv', index=False)

print("âœ“ LightGBM (WBCE) è®­ç»ƒå®Œæˆ")
print(f"éªŒè¯é›†QRT Score: {best_score:.4f}")
print(f"æäº¤æ–‡ä»¶: submit_lgb_wbce.csv")
print(f"æ¦‚ç‡æ–‡ä»¶: lightgbm_wbce_val_probs.csv, lightgbm_wbce_test_probs.csv") 