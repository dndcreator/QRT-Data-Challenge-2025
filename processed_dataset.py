import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from typing import Tuple, Optional
import pickle
import os


class ProcessedQRTDataset(Dataset):
    """
    Processed QRT Challenge Dataset Loader
    
    Uses the preprocessed data files generated by prepare_data.py
    
    Returns: (X, target_idx, target_sign, sample_weight)
    - X: standardized illiquid asset returns (RET_0 to RET_99)
    - target_idx: target liquid asset index (0-99)
    - target_sign: binary target sign (-1, 0, 1)
    - sample_weight: absolute value of RET_TARGET for weighted loss
    """
    
    def __init__(self, data_file: str, is_training: bool = True, 
                 transform=None, target_transform=None):
        """
        Args:
            data_file: path to processed data file (train_processed.csv or test_processed.csv)
            is_training: whether this is training data (has target variables)
            transform: optional transform for input features
            target_transform: optional transform for targets
        """
        self.data_file = data_file
        self.is_training = is_training
        self.transform = transform
        self.target_transform = target_transform
        
        # Load processed data
        print(f"Loading processed data from {data_file}...")
        self.data = pd.read_csv(data_file)
        
        # Extract feature columns (exclude RET_TARGET)
        self.feature_cols = [col for col in self.data.columns if col.startswith('RET_') and col != 'RET_TARGET']
        # 行业特征列
        self.industry_cols = [col for col in self.data.columns if col.startswith('CLASS_LEV')]
        # Convert to numpy arrays for faster access
        self.X = self.data[self.feature_cols].values.astype(np.float32)
        self.industry_ids = self.data[self.industry_cols].values.astype(np.int64) if self.industry_cols else None
        
        # Store input dimension for model configuration
        self.input_dim = len(self.feature_cols)
        self.ids = self.data['ID'].values  # 始终赋值

        # For training data, prepare targets and weights
        if self.is_training:
            self.target_idx = self.data['target_idx'].values.astype(np.int64)
            self.target_sign = self.data['target_sign'].values.astype(np.float32)
            self.sample_weights = np.abs(self.data['RET_TARGET']).values.astype(np.float32)
        else:
            self.target_idx = None
            self.target_sign = None
            self.sample_weights = None
        
        # Data validation
        print(f"Processed dataset loaded successfully:")
        print(f"  - Total samples: {len(self.data)}")
        print(f"  - Input features: {self.input_dim}")
        print(f"  - Feature value range: [{self.X.min():.4f}, {self.X.max():.4f}]")
        print(f"  - Feature mean/std: {self.X.mean():.4f}/{self.X.std():.4f}")
        
        if self.is_training:
            print(f"  - Target index range: {self.target_idx.min()} - {self.target_idx.max()}")
            # 修复目标符号分布显示
            try:
                unique_signs, counts = np.unique(self.target_sign, return_counts=True)
                sign_dist = dict(zip(unique_signs, counts))
                print(f"  - Target sign distribution: {sign_dist}")
            except Exception as e:
                print(f"  - Target sign distribution check failed: {e}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        X = self.X[idx]
        industry_id = self.industry_ids[idx] if self.industry_ids is not None else None
        if self.transform:
            X = self.transform(X)
        if self.is_training:
            target_idx = self.target_idx[idx]
            target_sign = self.target_sign[idx]
            sample_weight = self.sample_weights[idx]
            if self.target_transform:
                target_sign = self.target_transform(target_sign)
            return X, industry_id, target_idx, target_sign, sample_weight, self.ids[idx]
        else:
            return X, industry_id, self.ids[idx]
    
    def get_target_distribution(self):
        """Get distribution of target_idx values"""
        if self.is_training:
            return pd.Series(self.target_idx).value_counts().sort_index()
        return None
    
    def get_feature_stats(self):
        """Get basic statistics of input features"""
        return {
            'mean': self.X.mean(axis=0),
            'std': self.X.std(axis=0),
            'min': self.X.min(axis=0),
            'max': self.X.max(axis=0)
        }


def load_target_mapping(mapping_file: str = 'target_mapping.pkl'):
    """Load target mapping from pickle file"""
    if os.path.exists(mapping_file):
        with open(mapping_file, 'rb') as f:
            mapping = pickle.load(f)
        return mapping['target_to_idx'], mapping['idx_to_target']
    else:
        raise FileNotFoundError(f"Target mapping file {mapping_file} not found")


def create_processed_data_loaders(train_file: str = 'train_processed.csv',
                                test_file: str = 'test_processed.csv',
                                batch_size: int = 256,
                                train_split: float = 0.8, 
                                random_seed: int = 42):
    """
    Create train/validation/test data loaders using processed data
    
    Args:
        train_file: path to train_processed.csv
        test_file: path to test_processed.csv
        batch_size: batch size for training
        train_split: fraction of training data to use for training
        random_seed: random seed for reproducibility
    
    Returns:
        train_loader, val_loader, test_dataset
    """
    # Set random seed
    torch.manual_seed(random_seed)
    np.random.seed(random_seed)
    
    # Create full training dataset
    full_train_dataset = ProcessedQRTDataset(train_file, is_training=True)
    
    # Split into train and validation
    train_size = int(train_split * len(full_train_dataset))
    val_size = len(full_train_dataset) - train_size
    
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_train_dataset, [train_size, val_size]
    )
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=0,  # Set to 0 for Windows compatibility
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=0,
        pin_memory=True
    )
    
    # Create test dataset (no loader needed for inference)
    test_dataset = ProcessedQRTDataset(test_file, is_training=False)
    
    print(f"Training samples: {len(train_dataset)}")
    print(f"Validation samples: {len(val_dataset)}")
    print(f"Test samples: {len(test_dataset)}")
    
    return train_loader, val_loader, test_dataset


if __name__ == "__main__":
    # Test the processed dataset
    print("Testing ProcessedQRTDataset...")
    
    # Test training data
    if os.path.exists('train_processed.csv'):
        train_dataset = ProcessedQRTDataset('train_processed.csv', is_training=True)
        print(f"Training dataset: {len(train_dataset)} samples")
        
        # Test a sample
        sample = train_dataset[0]
        print(f"Sample format: {len(sample)} elements")
        if len(sample) == 4:
            X, target_idx, target_sign, weight = sample
            print(f"X shape: {X.shape}, target_idx: {target_idx}, target_sign: {target_sign}, weight: {weight}")
    
    # Test test data
    if os.path.exists('test_processed.csv'):
        test_dataset = ProcessedQRTDataset('test_processed.csv', is_training=False)
        print(f"Test dataset: {len(test_dataset)} samples")
        
        # Test a sample
        sample = test_dataset[0]
        print(f"Sample format: {len(sample)} elements")
        if len(sample) == 1:
            X = sample
            print(f"X shape: {X.shape}")
    
    # Test target mapping
    try:
        target_to_idx, idx_to_target = load_target_mapping()
        print(f"Target mapping loaded: {len(target_to_idx)} targets")
        print(f"Sample mapping: {list(target_to_idx.items())[:5]}")
    except FileNotFoundError as e:
        print(f"Warning: {e}") 