import pandas as pd
import numpy as np
import os
from sklearn.linear_model import LogisticRegression, RidgeCV, LassoCV
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

def qrt_score(y_true, y_pred, sample_weight):
    """QRT Score è®¡ç®—å‡½æ•°"""
    # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„ï¼Œé¿å…pandas Seriesç´¢å¼•é—®é¢˜
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    sample_weight = np.array(sample_weight)
    return np.sum((y_true == y_pred) * sample_weight) / np.sum(sample_weight)

def load_probabilities():
    """åŠ è½½æ¨¡å‹æ¦‚ç‡æ–‡ä»¶ï¼ˆæ”¯æŒWBCEå’ŒåŸºç¡€æ¨¡å‹ï¼‰"""
    print("=== åŠ è½½æ¨¡å‹æ¦‚ç‡æ–‡ä»¶ ===")
    
    # æ£€æŸ¥WBCEæ¨¡å‹æ¦‚ç‡æ–‡ä»¶
    wbce_files = [
        'lightgbm_wbce_val_probs.csv',
        'lightgbm_wbce_test_probs.csv',
        'catboost_wbce_val_probs.csv',
        'catboost_wbce_test_probs.csv'
    ]
    
    # æ£€æŸ¥åŸºç¡€ç‰¹å¾çš„æ¦‚ç‡æ–‡ä»¶
    basic_files = [
        'lightgbm_val_probs.csv',
        'lightgbm_test_probs.csv',
        'catboost_val_probs.csv',
        'catboost_test_probs.csv'
    ]
    
    # ä¼˜å…ˆä½¿ç”¨WBCEæ¨¡å‹
    if all(os.path.exists(f) for f in wbce_files):
        print("âœ“ ä½¿ç”¨WBCEæ¨¡å‹æ¦‚ç‡æ–‡ä»¶")
        lgb_val = pd.read_csv('lightgbm_wbce_val_probs.csv')
        cat_val = pd.read_csv('catboost_wbce_val_probs.csv')
        lgb_test = pd.read_csv('lightgbm_wbce_test_probs.csv')
        cat_test = pd.read_csv('catboost_wbce_test_probs.csv')
        
        print(f"LightGBM WBCEéªŒè¯é›†æ¦‚ç‡: {len(lgb_val)} æ ·æœ¬")
        print(f"CatBoost WBCEéªŒè¯é›†æ¦‚ç‡: {len(cat_val)} æ ·æœ¬")
        print(f"LightGBM WBCEæµ‹è¯•é›†æ¦‚ç‡: {len(lgb_test)} æ ·æœ¬")
        print(f"CatBoost WBCEæµ‹è¯•é›†æ¦‚ç‡: {len(cat_test)} æ ·æœ¬")
        
        print("âš ï¸ ä½¿ç”¨åŒæ¨¡å‹WBCEèåˆ")
        return lgb_val, cat_val, None, lgb_test, cat_test, None, 'wbce'
    
    # å¦‚æœæ²¡æœ‰WBCEæ–‡ä»¶ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹
    elif all(os.path.exists(f) for f in basic_files):
        print("âœ“ ä½¿ç”¨åŸºç¡€ç‰¹å¾çš„æ¦‚ç‡æ–‡ä»¶")
        lgb_val = pd.read_csv('lightgbm_val_probs.csv')
        cat_val = pd.read_csv('catboost_val_probs.csv')
        lgb_test = pd.read_csv('lightgbm_test_probs.csv')
        cat_test = pd.read_csv('catboost_test_probs.csv')
        
        print(f"LightGBMéªŒè¯é›†æ¦‚ç‡: {len(lgb_val)} æ ·æœ¬")
        print(f"CatBoostéªŒè¯é›†æ¦‚ç‡: {len(cat_val)} æ ·æœ¬")
        print(f"LightGBMæµ‹è¯•é›†æ¦‚ç‡: {len(lgb_test)} æ ·æœ¬")
        print(f"CatBoostæµ‹è¯•é›†æ¦‚ç‡: {len(cat_test)} æ ·æœ¬")
        
        print("âš ï¸ ä½¿ç”¨åŒæ¨¡å‹èåˆ")
        return lgb_val, cat_val, None, lgb_test, cat_test, None, 'basic'
    else:
        print("âŒ ç¼ºå°‘æ¦‚ç‡æ–‡ä»¶")
        return None, None, None, None, None, None, None

def weighted_average_ensemble(lgb_val, cat_val, lgb_test, cat_test, model_type='basic', weights=None):
    """åŠ æƒå¹³å‡èåˆï¼ˆæ”¯æŒWBCEå’ŒåŸºç¡€æ¨¡å‹ï¼‰"""
    print(f"\n=== åŠ æƒå¹³å‡èåˆ ({model_type.upper()}) ===")
    
    # åŠ è½½è®­ç»ƒæ•°æ®ä»¥è·å–æ ‡ç­¾å’Œæƒé‡ä¿¡æ¯
    train_data = pd.read_csv('train_processed.csv')
    
    # æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨æ£€æµ‹åˆ—å
    if model_type == 'wbce':
        lgb_col = 'lgb_wbce_prob' if 'lgb_wbce_prob' in lgb_val.columns else 'lightgbm_wbce_prob'
        cat_col = 'catboost_wbce_prob' if 'catboost_wbce_prob' in cat_val.columns else 'catboost_wbce_prob'
    else:
        lgb_col = 'lightgbm_prob' if 'lightgbm_prob' in lgb_val.columns else 'lightgbm_enhanced_prob'
        cat_col = 'catboost_prob' if 'catboost_prob' in cat_val.columns else 'catboost_enhanced_prob'
    
    # åŒæ¨¡å‹èåˆ - è‡ªåŠ¨ä¼˜åŒ–æƒé‡
    if weights is None:
        print("ğŸ” è‡ªåŠ¨ä¼˜åŒ–åŒæ¨¡å‹æƒé‡...")
        # ä»è®­ç»ƒæ•°æ®ä¸­è·å–å¯¹åº”çš„æ ‡ç­¾å’Œæƒé‡
        val_indices = lgb_val['ID'].values
        target_sign = train_data.loc[train_data['ID'].isin(val_indices), 'target_sign'].values
        ret_target = train_data.loc[train_data['ID'].isin(val_indices), 'RET_TARGET'].values
        weights = optimize_weights(lgb_val[lgb_col], cat_val[cat_col], target_sign, ret_target)
    
    print(f"ä½¿ç”¨ä¼˜åŒ–æƒé‡: LightGBM={weights[0]:.3f}, CatBoost={weights[1]:.3f}")
    print(f"ä½¿ç”¨åˆ—å: {lgb_col}, {cat_col}")
    
    # éªŒè¯é›†åŠ æƒå¹³å‡
    val_probs = (weights[0] * lgb_val[lgb_col] + 
                 weights[1] * cat_val[cat_col])
    
    # æµ‹è¯•é›†åŠ æƒå¹³å‡
    test_probs = (weights[0] * lgb_test[lgb_col] + 
                  weights[1] * cat_test[cat_col])
    
    return val_probs, test_probs

def optimize_weights(lgb_probs, cat_probs, target_sign, ret_target):
    """æ™ºèƒ½æƒé‡ä¼˜åŒ– - æœ€åä¸€æç­–ç•¥"""
    print("ğŸ” å¼€å§‹æ™ºèƒ½æƒé‡ä¼˜åŒ–...")
    
    # å‡†å¤‡æ ·æœ¬æƒé‡
    sample_weights = np.abs(ret_target)
    
    # ğŸš€ ç­–ç•¥1: åŸºäºæ¨¡å‹æ€§èƒ½çš„åŠ¨æ€æƒé‡
    lgb_score = 0
    cat_score = 0
    
    # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„å•ç‹¬æ€§èƒ½
    for thresh in np.arange(0.1, 0.9, 0.01):
        lgb_pred = (lgb_probs > thresh).astype(int) * 2 - 1
        cat_pred = (cat_probs > thresh).astype(int) * 2 - 1
        
        lgb_curr_score = qrt_score(target_sign * 2 - 1, lgb_pred, sample_weights)
        cat_curr_score = qrt_score(target_sign * 2 - 1, cat_pred, sample_weights)
        
        if lgb_curr_score > lgb_score:
            lgb_score = lgb_curr_score
        if cat_curr_score > cat_score:
            cat_score = cat_curr_score
    
    # åŸºäºæ€§èƒ½çš„æƒé‡
    total_score = lgb_score + cat_score
    if total_score > 0:
        perf_weights = [lgb_score / total_score, cat_score / total_score]
    else:
        perf_weights = [0.5, 0.5]
    
    # ğŸš€ ç­–ç•¥2: ç²¾ç»†ç½‘æ ¼æœç´¢
    best_score = -1
    best_weights = perf_weights  # ä»æ€§èƒ½æƒé‡å¼€å§‹
    
    # æ›´ç²¾ç»†çš„ç½‘æ ¼æœç´¢
    for w1 in np.arange(0.0, 1.01, 0.02):  # æ›´ç²¾ç»†çš„æ­¥é•¿
        w2 = 1.0 - w1
        if w2 < 0:
            continue
            
        # åŠ æƒå¹³å‡
        ensemble_probs = w1 * lgb_probs + w2 * cat_probs
        
        # è°ƒä¼˜é˜ˆå€¼
        best_thresh = 0.5
        best_model_score = -1
        for thresh in np.arange(0.1, 0.9, 0.01):
            ensemble_pred = (ensemble_probs > thresh).astype(int) * 2 - 1
            score = qrt_score(target_sign * 2 - 1, ensemble_pred, sample_weights)
            if score > best_model_score:
                best_model_score = score
                best_thresh = thresh
        
        if best_model_score > best_score:
            best_score = best_model_score
            best_weights = [w1, w2]
    
    print(f"âœ… æœ€ä¼˜æƒé‡: {best_weights}, QRT Score: {best_score:.4f}")
    print(f"ğŸ“Š æ¨¡å‹æ€§èƒ½: LightGBM={lgb_score:.4f}, CatBoost={cat_score:.4f}")
    return best_weights

def advanced_stacking(lgb_val, cat_val, lgb_test, cat_test, train_data, model_type='basic'):
    """é«˜çº§Stackingèåˆï¼ˆæ”¯æŒWBCEå’ŒåŸºç¡€æ¨¡å‹ï¼‰- åˆ›æ–°æ€§ä¼˜åŒ–"""
    print(f"\n=== é«˜çº§Stackingèåˆ ({model_type.upper()}) - åˆ›æ–°æ€§ä¼˜åŒ– ===")
    
    # æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨æ£€æµ‹åˆ—å
    if model_type == 'wbce':
        lgb_col = 'lgb_wbce_prob' if 'lgb_wbce_prob' in lgb_val.columns else 'lightgbm_wbce_prob'
        cat_col = 'catboost_wbce_prob' if 'catboost_wbce_prob' in cat_val.columns else 'catboost_wbce_prob'
    else:
        lgb_col = 'lightgbm_prob' if 'lightgbm_prob' in lgb_val.columns else 'lightgbm_enhanced_prob'
        cat_col = 'catboost_prob' if 'catboost_prob' in cat_val.columns else 'catboost_enhanced_prob'
    
    # ğŸš€ åˆ›æ–°æ€§ç‰¹å¾å·¥ç¨‹ - æ·»åŠ äº¤äº’ç‰¹å¾å’Œéçº¿æ€§å˜æ¢
    lgb_probs = lgb_val[lgb_col].values
    cat_probs = cat_val[cat_col].values
    
    # åŸºç¡€ç‰¹å¾
    X_train = np.column_stack([
        lgb_probs,
        cat_probs
    ])
    
    # ğŸ¯ åˆ›æ–°æ€§ç‰¹å¾å·¥ç¨‹
    # 1. äº¤äº’ç‰¹å¾
    interaction_feature = lgb_probs * cat_probs
    
    # 2. å·®å¼‚ç‰¹å¾
    diff_feature = lgb_probs - cat_probs
    
    # 3. æ¯”ç‡ç‰¹å¾
    ratio_feature = np.where(cat_probs > 0, lgb_probs / (cat_probs + 1e-8), 0)
    
    # 4. éçº¿æ€§å˜æ¢
    log_lgb = np.log(np.clip(lgb_probs, 1e-8, 1-1e-8))
    log_cat = np.log(np.clip(cat_probs, 1e-8, 1-1e-8))
    
    # 5. å¤šé¡¹å¼ç‰¹å¾
    lgb_squared = lgb_probs ** 2
    cat_squared = cat_probs ** 2
    
    # 6. ç»Ÿè®¡ç‰¹å¾
    mean_probs = (lgb_probs + cat_probs) / 2
    std_probs = np.sqrt(((lgb_probs - mean_probs) ** 2 + (cat_probs - mean_probs) ** 2) / 2)
    
    # ç»„åˆæ‰€æœ‰ç‰¹å¾
    X_train = np.column_stack([
        lgb_probs, cat_probs,  # åŸå§‹æ¦‚ç‡
        interaction_feature,    # äº¤äº’ç‰¹å¾
        diff_feature,          # å·®å¼‚ç‰¹å¾
        ratio_feature,         # æ¯”ç‡ç‰¹å¾
        log_lgb, log_cat,      # å¯¹æ•°å˜æ¢
        lgb_squared, cat_squared,  # å¤šé¡¹å¼ç‰¹å¾
        mean_probs, std_probs      # ç»Ÿè®¡ç‰¹å¾
    ])
    
    # å¯¹æµ‹è¯•é›†åº”ç”¨ç›¸åŒçš„ç‰¹å¾å·¥ç¨‹
    lgb_test_probs = lgb_test[lgb_col].values
    cat_test_probs = cat_test[cat_col].values
    
    interaction_test = lgb_test_probs * cat_test_probs
    diff_test = lgb_test_probs - cat_test_probs
    ratio_test = np.where(cat_test_probs > 0, lgb_test_probs / (cat_test_probs + 1e-8), 0)
    log_lgb_test = np.log(np.clip(lgb_test_probs, 1e-8, 1-1e-8))
    log_cat_test = np.log(np.clip(cat_test_probs, 1e-8, 1-1e-8))
    lgb_squared_test = lgb_test_probs ** 2
    cat_squared_test = cat_test_probs ** 2
    mean_test = (lgb_test_probs + cat_test_probs) / 2
    std_test = np.sqrt(((lgb_test_probs - mean_test) ** 2 + (cat_test_probs - mean_test) ** 2) / 2)
    
    X_test = np.column_stack([
        lgb_test_probs, cat_test_probs,
        interaction_test, diff_test, ratio_test,
        log_lgb_test, log_cat_test,
        lgb_squared_test, cat_squared_test,
        mean_test, std_test
    ])

    print(f"ğŸš€ åˆ›æ–°æ€§ç‰¹å¾å·¥ç¨‹: åŸå§‹2ç»´ â†’ å¢å¼º10ç»´ç‰¹å¾")
    print(f"ç‰¹å¾åŒ…æ‹¬: åŸå§‹æ¦‚ç‡ã€äº¤äº’ã€å·®å¼‚ã€æ¯”ç‡ã€å¯¹æ•°ã€å¤šé¡¹å¼ã€ç»Ÿè®¡ç‰¹å¾")
    
    # å‡†å¤‡æ ‡ç­¾å’Œæ ·æœ¬æƒé‡
    y_train = train_data.loc[lgb_val['ID'], 'target_sign'].values
    y_train = (y_train > 0).astype(int)
    sample_weights = train_data.loc[lgb_val['ID'], 'RET_TARGET'].abs().values
    
    # è®­ç»ƒå¤šä¸ªStackingæ¨¡å‹ - åˆ›æ–°æ€§å…ƒæ¨¡å‹ç»„åˆ
    models = {}
    
    # 1. Logistic Regression - åŸºç¡€çº¿æ€§æ¨¡å‹
    lr_model = LogisticRegression(random_state=42, max_iter=1000, C=1.0)
    lr_model.fit(X_train, y_train, sample_weight=sample_weights)
    models['LogisticRegression'] = lr_model
    
    # 2. Ridge Regression - æ­£åˆ™åŒ–çº¿æ€§æ¨¡å‹
    ridge_model = RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0, 100.0])
    ridge_model.fit(X_train, y_train, sample_weight=sample_weights)
    models['Ridge'] = ridge_model
    
    # 3. Lasso Regression - ç¨€ç–çº¿æ€§æ¨¡å‹
    lasso_model = LassoCV(cv=5, random_state=42, max_iter=1000, alphas=[0.001, 0.01, 0.1, 1.0])
    lasso_model.fit(X_train, y_train, sample_weight=sample_weights)
    models['Lasso'] = lasso_model
    
    # 4. Elastic Net - å¼¹æ€§ç½‘ç»œ (ç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å…æ”¶æ•›é—®é¢˜)
    from sklearn.linear_model import ElasticNet
    elastic_model = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42, max_iter=2000)
    elastic_model.fit(X_train, y_train, sample_weight=sample_weights)
    models['ElasticNet'] = elastic_model
    
    # 5. æ”¯æŒå‘é‡æœº - éçº¿æ€§æ¨¡å‹ (ç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å…é•¿æ—¶é—´è®­ç»ƒ)
    from sklearn.svm import SVC
    svm_model = SVC(kernel='rbf', probability=True, random_state=42, C=0.1, gamma='scale', max_iter=1000)
    svm_model.fit(X_train, y_train, sample_weight=sample_weights)
    models['SVM'] = svm_model
    
    # é¢„æµ‹å¹¶è¯„ä¼° - æ™ºèƒ½é¢„æµ‹å¤„ç†
    results = {}
    for name, model in models.items():
        # éªŒè¯é›†é¢„æµ‹
        if hasattr(model, 'predict_proba'):
            val_probs = model.predict_proba(X_train)[:, 1]
        else:
            val_probs = model.predict(X_train)
            # å¯¹äºå›å½’æ¨¡å‹ï¼Œéœ€è¦å°†è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡
            if name in ['Ridge', 'Lasso', 'ElasticNet']:
                val_probs = 1 / (1 + np.exp(-val_probs))  # sigmoidè½¬æ¢
            val_probs = np.clip(val_probs, 0, 1)  # ç¡®ä¿æ¦‚ç‡åœ¨[0,1]èŒƒå›´å†…
        
        # æµ‹è¯•é›†é¢„æµ‹
        if hasattr(model, 'predict_proba'):
            test_probs = model.predict_proba(X_test)[:, 1]
        else:
            test_probs = model.predict(X_test)
            # å¯¹äºå›å½’æ¨¡å‹ï¼Œéœ€è¦å°†è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡
            if name in ['Ridge', 'Lasso', 'ElasticNet']:
                test_probs = 1 / (1 + np.exp(-test_probs))  # sigmoidè½¬æ¢
            test_probs = np.clip(test_probs, 0, 1)  # ç¡®ä¿æ¦‚ç‡åœ¨[0,1]èŒƒå›´å†…
        
        results[name] = {
            'val_probs': val_probs,
            'test_probs': test_probs,
            'model': model
        }
        
        # æ‰“å°æ¨¡å‹ç³»æ•° - æ™ºèƒ½ç³»æ•°å¤„ç†
        if hasattr(model, 'coef_'):
            coef = model.coef_
            if coef.ndim == 2:
                # å¯¹äºå¤šç‰¹å¾æ¨¡å‹ï¼Œåªæ˜¾ç¤ºå‰ä¸¤ä¸ªç³»æ•°
                if coef.shape[1] >= 2:
                    lgb_coef = float(coef[0, 0])
                    cat_coef = float(coef[0, 1])
                else:
                    lgb_coef = float(coef[0, 0]) if coef.shape[1] > 0 else 0
                    cat_coef = 0
            else:
                # 1Dæ•°ç»„
                if len(coef) >= 2:
                    lgb_coef = float(coef[0])
                    cat_coef = float(coef[1])
                else:
                    lgb_coef = float(coef[0]) if len(coef) > 0 else 0
                    cat_coef = 0
            print(f"  {name} - ç³»æ•°: LightGBM={lgb_coef:.4f}, CatBoost={cat_coef:.4f}")
        elif hasattr(model, 'alpha_'):
            coef = model.coef_
            if coef.ndim == 2:
                lgb_coef = float(coef[0, 0]) if coef.shape[1] > 0 else 0
                cat_coef = float(coef[0, 1]) if coef.shape[1] > 1 else 0
            else:
                lgb_coef = float(coef[0]) if len(coef) > 0 else 0
                cat_coef = float(coef[1]) if len(coef) > 1 else 0
            print(f"  {name} - Alpha: {model.alpha_:.4f}, ç³»æ•°: LightGBM={lgb_coef:.4f}, CatBoost={cat_coef:.4f}")
        else:
            print(f"  {name} - éçº¿æ€§æ¨¡å‹ï¼Œæ— çº¿æ€§ç³»æ•°")
            
    # é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯é›†QRT Scoreï¼‰
    best_score = -1
    best_model_name = None
    best_val_probs = None
    best_test_probs = None
    
    for name, result in results.items():
        val_probs = result['val_probs']
        
        # è°ƒä¼˜é˜ˆå€¼
        best_thresh = 0.5
        best_model_score = -1
        for thresh in np.arange(0.1, 0.9, 0.01):
            val_pred = (val_probs > thresh).astype(int) * 2 - 1
            score = qrt_score(y_train * 2 - 1, val_pred, sample_weights)
            if score > best_model_score:
                best_model_score = score
                best_thresh = thresh
            
        print(f"  {name} - éªŒè¯é›†QRT Score: {best_model_score:.4f}")
            
        if best_model_score > best_score:
            best_score = best_model_score
            best_model_name = name
            best_val_probs = val_probs
            best_test_probs = result['test_probs']
    
    print(f"\nğŸ¯ æœ€ä½³æ¨¡å‹: {best_model_name} (QRT Score: {best_score:.4f})")
    
    # ğŸš€ åˆ›æ–°æ€§é›†æˆæŠ•ç¥¨æœºåˆ¶
    print(f"\nğŸš€ åˆ›æ–°æ€§é›†æˆæŠ•ç¥¨æœºåˆ¶")
    
    # è®¡ç®—æ‰€æœ‰æ¨¡å‹çš„åŠ æƒå¹³å‡
    all_val_probs = []
    all_test_probs = []
    model_scores = []
    
    for name, result in results.items():
        val_probs = result['val_probs']
        
        # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„QRT Score
        best_thresh = 0.5
        best_model_score = -1
        for thresh in np.arange(0.1, 0.9, 0.01):
            val_pred = (val_probs > thresh).astype(int) * 2 - 1
            score = qrt_score(y_train * 2 - 1, val_pred, sample_weights)
            if score > best_model_score:
                best_model_score = score
                best_thresh = thresh
        
        all_val_probs.append(val_probs)
        all_test_probs.append(result['test_probs'])
        model_scores.append(best_model_score)
    
    # åŸºäºæ€§èƒ½çš„åŠ æƒå¹³å‡
    model_scores = np.array(model_scores)
    model_weights = model_scores / np.sum(model_scores) if np.sum(model_scores) > 0 else np.ones(len(model_scores)) / len(model_scores)
    
    # è®¡ç®—é›†æˆæŠ•ç¥¨ç»“æœ
    ensemble_val_probs = np.zeros_like(all_val_probs[0])
    ensemble_test_probs = np.zeros_like(all_test_probs[0])
    
    for i, (val_probs, test_probs, weight) in enumerate(zip(all_val_probs, all_test_probs, model_weights)):
        ensemble_val_probs += weight * val_probs
        ensemble_test_probs += weight * test_probs
    
    # è¯„ä¼°é›†æˆæŠ•ç¥¨ç»“æœ
    best_thresh = 0.5
    best_ensemble_score = -1
    for thresh in np.arange(0.1, 0.9, 0.01):
        ensemble_pred = (ensemble_val_probs > thresh).astype(int) * 2 - 1
        score = qrt_score(y_train * 2 - 1, ensemble_pred, sample_weights)
        if score > best_ensemble_score:
            best_ensemble_score = score
            best_thresh = thresh
    
    print(f"ğŸ¯ é›†æˆæŠ•ç¥¨QRT Score: {best_ensemble_score:.4f}")
    print(f"ğŸ“Š æ¨¡å‹æƒé‡: {dict(zip(results.keys(), model_weights))}")
    
    # é€‰æ‹©æœ€ä½³ç»“æœ
    if best_ensemble_score > best_score:
        print(f"âœ… é›†æˆæŠ•ç¥¨ä¼˜äºæœ€ä½³å•æ¨¡å‹ï¼Œä½¿ç”¨é›†æˆç»“æœ")
        return ensemble_val_probs, ensemble_test_probs, "EnsembleVoting"
    else:
        print(f"âœ… æœ€ä½³å•æ¨¡å‹ä¼˜äºé›†æˆæŠ•ç¥¨ï¼Œä½¿ç”¨å•æ¨¡å‹ç»“æœ")
        return best_val_probs, best_test_probs, best_model_name

def logistic_regression_stacking(lgb_val, cat_val, lgb_test, cat_test, train_data, model_type='basic'):
    """Logistic Regression Stackingï¼ˆæ”¯æŒWBCEå’ŒåŸºç¡€æ¨¡å‹ï¼‰"""
    print(f"\n=== Logistic Regression Stacking ({model_type.upper()}) ===")
    
    # æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨æ£€æµ‹åˆ—å
    if model_type == 'wbce':
        lgb_col = 'lgb_wbce_prob' if 'lgb_wbce_prob' in lgb_val.columns else 'lightgbm_wbce_prob'
        cat_col = 'catboost_wbce_prob' if 'catboost_wbce_prob' in cat_val.columns else 'catboost_wbce_prob'
    else:
        lgb_col = 'lightgbm_prob' if 'lightgbm_prob' in lgb_val.columns else 'lightgbm_enhanced_prob'
        cat_col = 'catboost_prob' if 'catboost_prob' in cat_val.columns else 'catboost_enhanced_prob'
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X_train = np.column_stack([
        lgb_val[lgb_col].values,
        cat_val[cat_col].values
    ])
    
    X_test = np.column_stack([
        lgb_test[lgb_col].values,
        cat_test[cat_col].values
    ])
    
    # å‡†å¤‡æ ‡ç­¾å’Œæƒé‡
    y_train = train_data.loc[lgb_val['ID'], 'target_sign'].values
    y_train = (y_train > 0).astype(int)
    sample_weights = train_data.loc[lgb_val['ID'], 'RET_TARGET'].abs().values
    
    # è®­ç»ƒLogistic Regression
    lr_model = LogisticRegression(random_state=42, max_iter=1000)
    lr_model.fit(X_train, y_train, sample_weight=sample_weights)
    
    # é¢„æµ‹
    val_probs = lr_model.predict_proba(X_train)[:, 1]
    test_probs = lr_model.predict_proba(X_test)[:, 1]
    
    print(f"Logistic Regressionç³»æ•°: LightGBM={float(lr_model.coef_[0][0]):.4f}, CatBoost={float(lr_model.coef_[0][1]):.4f}")
    
    return val_probs, test_probs

def evaluate_ensemble(val_probs, train_data, val_indices, method_name):
    """è¯„ä¼°é›†æˆæ¨¡å‹"""
    print(f"\n=== è¯„ä¼° {method_name} ===")
    
    # è·å–éªŒè¯é›†æ ‡ç­¾å’Œæƒé‡
    y_val = train_data.loc[val_indices, 'target_sign'].values
    y_val = (y_val > 0).astype(int)
    val_weights = train_data.loc[val_indices, 'RET_TARGET'].abs().values
    
    # è°ƒä¼˜é˜ˆå€¼
    best_score = -1
    best_thresh = 0.5
    for thresh in np.arange(0.1, 0.9, 0.01):
        val_pred = (val_probs > thresh).astype(int) * 2 - 1
        score = qrt_score(y_val * 2 - 1, val_pred, val_weights)
        if score > best_score:
            best_score = score
            best_thresh = thresh
    
    print(f"æœ€ä¼˜é˜ˆå€¼: {best_thresh:.2f}")
    print(f"éªŒè¯é›†QRT Score: {best_score:.4f}")
    
    return best_score, best_thresh
    
def create_submission(test_probs, threshold, method_name, model_type='basic'):
    """åˆ›å»ºæäº¤æ–‡ä»¶"""
    test_pred = (test_probs > threshold).astype(int) * 2 - 1
    
    # è¯»å–æµ‹è¯•é›†ID
    test = pd.read_csv('test_processed.csv')
    
    submit = pd.DataFrame({
        'ID': test['ID'],
        'RET_TARGET': test_pred
    })
    
    # æ ¹æ®æ¨¡å‹ç±»å‹å‘½åæ–‡ä»¶
    if model_type == 'wbce':
        filename = f'submit_{method_name}_wbce.csv'
    else:
        filename = f'submit_{method_name}.csv'
    
    submit.to_csv(filename, index=False)
    print(f"æäº¤æ–‡ä»¶å·²ä¿å­˜: {filename}")
    
    return filename

def plot_probability_comparison(lgb_val, cat_val, val_probs_weighted, val_probs_stacking, model_type='basic'):
    """ç»˜åˆ¶æ¦‚ç‡å¯¹æ¯”å›¾"""
    try:
        plt.figure(figsize=(15, 5))
        
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆ—å
        if model_type == 'wbce':
            lgb_col = 'lgb_wbce_prob' if 'lgb_wbce_prob' in lgb_val.columns else 'lightgbm_wbce_prob'
            cat_col = 'catboost_wbce_prob' if 'catboost_wbce_prob' in cat_val.columns else 'catboost_wbce_prob'
        else:
            lgb_col = 'lightgbm_prob' if 'lightgbm_prob' in lgb_val.columns else 'lightgbm_enhanced_prob'
            cat_col = 'catboost_prob' if 'catboost_prob' in cat_val.columns else 'catboost_enhanced_prob'
        
        # å­å›¾1: åŸå§‹æ¨¡å‹æ¦‚ç‡åˆ†å¸ƒ
        plt.subplot(1, 3, 1)
        plt.hist(lgb_val[lgb_col], bins=50, alpha=0.7, label='LightGBM', density=True)
        plt.hist(cat_val[cat_col], bins=50, alpha=0.7, label='CatBoost', density=True)
        plt.xlabel('Probability')
        plt.ylabel('Density')
        plt.title('Original Model Probabilities')
        plt.legend()
        
        # å­å›¾2: åŠ æƒå¹³å‡æ¦‚ç‡åˆ†å¸ƒ
        plt.subplot(1, 3, 2)
        plt.hist(val_probs_weighted, bins=50, alpha=0.7, color='green', density=True)
        plt.xlabel('Probability')
        plt.ylabel('Density')
        plt.title('Weighted Average Probabilities')
        
        # å­å›¾3: Stackingæ¦‚ç‡åˆ†å¸ƒ
        plt.subplot(1, 3, 3)
        plt.hist(val_probs_stacking, bins=50, alpha=0.7, color='red', density=True)
        plt.xlabel('Probability')
        plt.ylabel('Density')
        plt.title('Stacking Probabilities')
        
        plt.tight_layout()
        
        # æ ¹æ®æ¨¡å‹ç±»å‹å‘½åæ–‡ä»¶
        if model_type == 'wbce':
            filename = 'ensemble_probability_comparison_wbce.png'
        else:
            filename = 'ensemble_probability_comparison.png'
        
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.show()
        print(f"æ¦‚ç‡å¯¹æ¯”å›¾å·²ä¿å­˜: {filename}")
        
    except Exception as e:
        print(f"ç»˜å›¾å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ é«˜çº§Stackingé›†æˆå­¦ä¹ ")
    print("=" * 50)
    
    # åŠ è½½æ¦‚ç‡æ–‡ä»¶
    lgb_val, cat_val, _, lgb_test, cat_test, _, model_type = load_probabilities()
    
    if lgb_val is None:
        print("âŒ æ— æ³•åŠ è½½æ¦‚ç‡æ–‡ä»¶ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return
    
    # åŠ è½½è®­ç»ƒæ•°æ®ç”¨äºè¯„ä¼°
    train_data = pd.read_csv('train_processed.csv')
    val_indices = lgb_val['ID'].values
    
    print(f"\nğŸ“Š ä½¿ç”¨æ¨¡å‹ç±»å‹: {model_type.upper()}")
    
    # 1. åŠ æƒå¹³å‡èåˆ
    val_probs_weighted, test_probs_weighted = weighted_average_ensemble(
        lgb_val, cat_val, lgb_test, cat_test, model_type
    )
    
    weighted_score, weighted_thresh = evaluate_ensemble(
        val_probs_weighted, train_data, val_indices, "åŠ æƒå¹³å‡"
    )
    
    create_submission(test_probs_weighted, weighted_thresh, "åŠ æƒå¹³å‡", model_type)
    
    # 2. é«˜çº§Stackingèåˆ
    val_probs_stacking, test_probs_stacking, best_model_name = advanced_stacking(
        lgb_val, cat_val, lgb_test, cat_test, train_data, model_type
    )
    
    stacking_score, stacking_thresh = evaluate_ensemble(
        val_probs_stacking, train_data, val_indices, f"é«˜çº§Stacking({best_model_name})"
    )
    
    create_submission(test_probs_stacking, stacking_thresh, "é«˜çº§stacking", model_type)
    
    # 3. åŒæ¨¡å‹Logistic Regression Stacking
    val_probs_lr, test_probs_lr = logistic_regression_stacking(
        lgb_val, cat_val, lgb_test, cat_test, train_data, model_type
    )
    
    lr_score, lr_thresh = evaluate_ensemble(
        val_probs_lr, train_data, val_indices, "LogisticRegression"
    )
    
    create_submission(test_probs_lr, lr_thresh, "logistic_regression", model_type)
    
    # 4. ç»˜åˆ¶æ¦‚ç‡å¯¹æ¯”å›¾
    plot_probability_comparison(lgb_val, cat_val, val_probs_weighted, val_probs_stacking, model_type)
    
    # 5. æ€»ç»“ç»“æœ
    print("\n" + "=" * 50)
    print("ğŸ¯ é›†æˆå­¦ä¹ ç»“æœæ€»ç»“")
    print("=" * 50)
    
    methods = [
        ("åŠ æƒå¹³å‡", weighted_score),
        (f"é«˜çº§Stacking({best_model_name})", stacking_score),
        ("LogisticRegression", lr_score)
    ]
    
    for method, score in methods:
        print(f"  {method}: QRT Score = {score:.4f}")
    
    best_method, best_score = max(methods, key=lambda x: x[1])
    print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {best_method} (QRT Score: {best_score:.4f})")
    
    if model_type == 'wbce':
        print("âœ… ä½¿ç”¨WBCEä¼˜åŒ–æ¨¡å‹å®Œæˆ")
    else:
        print("âœ… ä½¿ç”¨åŸºç¡€æ¨¡å‹å®Œæˆ")
    
    print("\nğŸ‰ æ‰€æœ‰é›†æˆæ–¹æ³•å®Œæˆï¼")

if __name__ == "__main__":
    main() 